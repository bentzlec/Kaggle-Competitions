{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-15T00:15:30.014450Z","iopub.execute_input":"2023-06-15T00:15:30.014840Z","iopub.status.idle":"2023-06-15T00:15:30.022975Z","shell.execute_reply.started":"2023-06-15T00:15:30.014810Z","shell.execute_reply":"2023-06-15T00:15:30.021765Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\n\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:30.025778Z","iopub.execute_input":"2023-06-15T00:15:30.026226Z","iopub.status.idle":"2023-06-15T00:15:30.070686Z","shell.execute_reply.started":"2023-06-15T00:15:30.026187Z","shell.execute_reply":"2023-06-15T00:15:30.069450Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleansing","metadata":{}},{"cell_type":"code","source":"#Original Training Data\ntrain['text'][:5]","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:30.072359Z","iopub.execute_input":"2023-06-15T00:15:30.072692Z","iopub.status.idle":"2023-06-15T00:15:30.080559Z","shell.execute_reply.started":"2023-06-15T00:15:30.072658Z","shell.execute_reply":"2023-06-15T00:15:30.079668Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0    Our Deeds are the Reason of this #earthquake M...\n1               Forest fire near La Ronge Sask. Canada\n2    All residents asked to 'shelter in place' are ...\n3    13,000 people receive #wildfires evacuation or...\n4    Just got sent this photo from Ruby #Alaska as ...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\ntrain['text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:30.082512Z","iopub.execute_input":"2023-06-15T00:15:30.082998Z","iopub.status.idle":"2023-06-15T00:15:30.582933Z","shell.execute_reply.started":"2023-06-15T00:15:30.082948Z","shell.execute_reply":"2023-06-15T00:15:30.581782Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0    our deeds are the reason of this earthquake ma...\n1                forest fire near la ronge sask canada\n2    all residents asked to shelter in place are be...\n3     people receive wildfires evacuation orders in...\n4    just got sent this photo from ruby alaska as s...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:30.598148Z","iopub.execute_input":"2023-06-15T00:15:30.598603Z","iopub.status.idle":"2023-06-15T00:15:30.707110Z","shell.execute_reply.started":"2023-06-15T00:15:30.598565Z","shell.execute_reply":"2023-06-15T00:15:30.705785Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0    [our, deeds, are, the, reason, of, this, earth...\n1        [forest, fire, near, la, ronge, sask, canada]\n2    [all, residents, asked, to, shelter, in, place...\n3    [people, receive, wildfires, evacuation, order...\n4    [just, got, sent, this, photo, from, ruby, ala...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"#Removing Stop Words\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\n\n# After preprocessing, the text format\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:30.708625Z","iopub.execute_input":"2023-06-15T00:15:30.709057Z","iopub.status.idle":"2023-06-15T00:15:51.074831Z","shell.execute_reply.started":"2023-06-15T00:15:30.709017Z","shell.execute_reply":"2023-06-15T00:15:51.073563Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  [deeds, reason, earthquake, may, allah, forgiv...   \n1   4     NaN      NaN      [forest, fire, near, la, ronge, sask, canada]   \n2   5     NaN      NaN  [residents, asked, shelter, place, notified, o...   \n3   6     NaN      NaN  [people, receive, wildfires, evacuation, order...   \n4   7     NaN      NaN  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[residents, asked, shelter, place, notified, o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[people, receive, wildfires, evacuation, order...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:51.112676Z","iopub.execute_input":"2023-06-15T00:15:51.113701Z","iopub.status.idle":"2023-06-15T00:15:51.120180Z","shell.execute_reply.started":"2023-06-15T00:15:51.113665Z","shell.execute_reply":"2023-06-15T00:15:51.119229Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# TFIDF\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:51.121582Z","iopub.execute_input":"2023-06-15T00:15:51.121930Z","iopub.status.idle":"2023-06-15T00:15:51.533113Z","shell.execute_reply.started":"2023-06-15T00:15:51.121901Z","shell.execute_reply":"2023-06-15T00:15:51.531650Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"clf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('XGBoost', scores)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:15:51.534881Z","iopub.execute_input":"2023-06-15T00:15:51.536199Z","iopub.status.idle":"2023-06-15T00:16:05.932090Z","shell.execute_reply.started":"2023-06-15T00:15:51.536157Z","shell.execute_reply":"2023-06-15T00:16:05.930917Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"XGBoost [0.48498331 0.32860041 0.42528736 0.42182581 0.53256705]\n","output_type":"stream"}]},{"cell_type":"code","source":"svm = SVC(kernel=\"rbf\", gamma=0.5, C=1.0)\nscores = model_selection.cross_val_score(svm, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('SVM', scores)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:16:05.933746Z","iopub.execute_input":"2023-06-15T00:16:05.934435Z","iopub.status.idle":"2023-06-15T00:16:27.350211Z","shell.execute_reply.started":"2023-06-15T00:16:05.934396Z","shell.execute_reply":"2023-06-15T00:16:27.349242Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"SVM [0.55005269 0.46507177 0.52730697 0.45580405 0.64072398]\n","output_type":"stream"}]},{"cell_type":"code","source":"clf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Naive Bayes:', scores)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:16:27.351797Z","iopub.execute_input":"2023-06-15T00:16:27.352835Z","iopub.status.idle":"2023-06-15T00:16:27.404109Z","shell.execute_reply.started":"2023-06-15T00:16:27.352800Z","shell.execute_reply":"2023-06-15T00:16:27.403080Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Naive Bayes: [0.57590597 0.57092511 0.61135371 0.5962963  0.7393745 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"target = train['target']\nx, x_test, y, y_test = train_test_split(train_tfidf,target,test_size=0.2,train_size=0.8, random_state = 0)\n\nclf = MultinomialNB(alpha=1).fit(x, y)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:16:27.408314Z","iopub.execute_input":"2023-06-15T00:16:27.408725Z","iopub.status.idle":"2023-06-15T00:16:27.423759Z","shell.execute_reply.started":"2023-06-15T00:16:27.408693Z","shell.execute_reply":"2023-06-15T00:16:27.422678Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"preds = clf.predict(test_tfidf)\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['target']= preds\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T00:16:27.425366Z","iopub.execute_input":"2023-06-15T00:16:27.425867Z","iopub.status.idle":"2023-06-15T00:16:27.452054Z","shell.execute_reply.started":"2023-06-15T00:16:27.425824Z","shell.execute_reply":"2023-06-15T00:16:27.450661Z"},"trusted":true},"execution_count":31,"outputs":[]}]}